{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"22-[seg]13-densenet169_[class]8-resnet50_224x224.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"cells":[{"cell_type":"code","metadata":{"id":"1KWL1G5sXwFr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"6f186243-a739-4aec-9d42-0e2a0d784dee","executionInfo":{"status":"ok","timestamp":1571963748594,"user_tz":180,"elapsed":27946,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DDDjJ17CXziL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"58111571-f4d3-46c0-8cd2-c07809855277","executionInfo":{"status":"ok","timestamp":1571963909649,"user_tz":180,"elapsed":13718,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}}},"source":["!unzip -q '/content/drive/My Drive/Colab Notebooks/[Kaggle] Understanding Clouds from Satellite Images/Data/test_images256x384.zip'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["replace test_images256x384/2698804.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yxvzFnySHdqd"},"source":["### Dependencies"]},{"cell_type":"code","metadata":{"id":"rl40mXGJXmpB","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"93419ee4-3a37-455c-a1f2-636ce7a796eb","executionInfo":{"status":"ok","timestamp":1571963802885,"user_tz":180,"elapsed":25146,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}}},"source":["#@title\n","# Dependencies\n","import os\n","import cv2\n","import math\n","import random\n","import shutil\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from skimage import exposure\n","import multiprocessing as mp\n","import albumentations as albu\n","import matplotlib.pyplot as plt\n","from tensorflow import set_random_seed\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score, fbeta_score\n","from keras import backend as K\n","from keras.utils import Sequence\n","from keras.layers import Input, average\n","from keras import optimizers, applications\n","from keras.models import Model, load_model\n","from keras.losses import binary_crossentropy\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Dense, GlobalAveragePooling2D, Input\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","# Required repositories\n","os.system('pip install segmentation-models')\n","os.system('pip install keras-rectified-adam')\n","os.system('pip install tta-wrapper')\n","\n","from keras_radam import RAdam\n","import segmentation_models as sm\n","from tta_wrapper import tta_segmentation\n","\n","# Misc\n","def seed_everything(seed=0):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    set_random_seed(seed)\n","    \n","    \n","# Segmentation related\n","def rle_decode(mask_rle, shape=(1400, 2100)):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape, order='F')  # Needed to align to RLE direction\n","\n","def rle_to_mask(rle_string, height, width):\n","    rows, cols = height, width\n","    \n","    if rle_string == -1:\n","        return np.zeros((height, width))\n","    else:\n","        rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n","        rle_pairs = np.array(rle_numbers).reshape(-1,2)\n","        img = np.zeros(rows*cols, dtype=np.uint8)\n","        for index, length in rle_pairs:\n","            index -= 1\n","            img[index:index+length] = 255\n","        img = img.reshape(cols,rows)\n","        img = img.T\n","        return img\n","    \n","def get_mask_area(df, index, column_name, shape=(1400, 2100)):\n","    rle = df.loc[index][column_name]\n","    try:\n","        math.isnan(rle)\n","        np_mask = np.zeros((shape[0], shape[1], 3))\n","    except:\n","        np_mask = rle_to_mask(rle, shape[0], shape[1])\n","        np_mask = np.clip(np_mask, 0, 1)\n","        \n","    return int(np.sum(np_mask))\n","\n","def np_resize(img, input_shape):\n","    \"\"\"\n","    Reshape a numpy array, which is input_shape=(height, width), \n","    as opposed to input_shape=(width, height) for cv2\n","    \"\"\"\n","    height, width = input_shape\n","    return cv2.resize(img, (width, height))\n","    \n","def mask2rle(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels= img.T.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n","\n","def build_rles(masks, reshape=None):\n","    width, height, depth = masks.shape\n","    rles = []\n","    \n","    for i in range(depth):\n","        mask = masks[:, :, i]\n","        \n","        if reshape:\n","            mask = mask.astype(np.float32)\n","            mask = np_resize(mask, reshape).astype(np.int64)\n","        \n","        rle = mask2rle(mask)\n","        rles.append(rle)\n","        \n","    return rles\n","\n","def build_masks(rles, input_shape, reshape=None):\n","    depth = len(rles)\n","    if reshape is None:\n","        masks = np.zeros((*input_shape, depth))\n","    else:\n","        masks = np.zeros((*reshape, depth))\n","    \n","    for i, rle in enumerate(rles):\n","        if type(rle) is str:\n","            if reshape is None:\n","                masks[:, :, i] = rle2mask(rle, input_shape)\n","            else:\n","                mask = rle2mask(rle, input_shape)\n","                reshaped_mask = np_resize(mask, reshape)\n","                masks[:, :, i] = reshaped_mask\n","    \n","    return masks\n","\n","def rle2mask(rle, input_shape):\n","    width, height = input_shape[:2]\n","    mask = np.zeros( width*height ).astype(np.uint8)\n","    array = np.asarray([int(x) for x in rle.split()])\n","    starts = array[0::2]\n","    lengths = array[1::2]\n","\n","    current_position = 0\n","    for index, start in enumerate(starts):\n","        mask[int(start):int(start+lengths[index])] = 1\n","        current_position += lengths[index]\n","        \n","    return mask.reshape(height, width).T\n","\n","def dice_coefficient(y_true, y_pred):\n","    y_true = np.asarray(y_true).astype(np.bool)\n","    y_pred = np.asarray(y_pred).astype(np.bool)\n","    intersection = np.logical_and(y_true, y_pred)\n","    return (2. * intersection.sum()) / (y_true.sum() + y_pred.sum())\n","\n","def dice_coef(y_true, y_pred, smooth=1):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    \n","# Data pre-process\n","def preprocess_image(image_id, base_path, save_path, HEIGHT, WIDTH):\n","    image = cv2.imread(base_path + image_id)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = cv2.resize(image, (WIDTH, HEIGHT))\n","    cv2.imwrite(save_path + image_id, image)\n","    \n","def pre_process_set(df, preprocess_fn):\n","    n_cpu = mp.cpu_count()\n","    df_n_cnt = df.shape[0]//n_cpu\n","    pool = mp.Pool(n_cpu)\n","    \n","    dfs = [df.iloc[df_n_cnt*i:df_n_cnt*(i+1)] for i in range(n_cpu)]\n","    dfs[-1] = df.iloc[df_n_cnt*(n_cpu-1):]\n","    res = pool.map(preprocess_fn, [x_df for x_df in dfs])\n","    pool.close()\n","        \n","# def preprocess_data(df, HEIGHT=HEIGHT, WIDTH=WIDTH):\n","#     df = df.reset_index()\n","#     for i in range(df.shape[0]):\n","#         item = df.iloc[i]\n","#         image_id = item['image']\n","#         item_set = item['set']\n","#         if item_set == 'train':\n","#             preprocess_image(image_id, train_base_path, train_images_dest_path, HEIGHT, WIDTH)\n","#         if item_set == 'validation':\n","#             preprocess_image(image_id, train_base_path, validation_images_dest_path, HEIGHT, WIDTH)\n","#         if item_set == 'test':\n","#             preprocess_image(image_id, test_base_path, test_images_dest_path, HEIGHT, WIDTH)\n","\n","# Model evaluation\n","def get_metrics_classification(df, preds, label_columns, threshold=0.5, show_report=True):\n","  accuracy = []\n","  precision = []\n","  recall = []\n","  f_score = []\n","  for index, label in enumerate(label_columns):\n","    print('Metrics for: %s' % label)\n","    if show_report:\n","      print(classification_report(df[label], (preds[:,index] > threshold).astype(int), output_dict=False))\n","    metrics = classification_report(df[label], (preds[:,index] > threshold).astype(int), output_dict=True)\n","    accuracy.append(metrics['accuracy'])\n","    precision.append(metrics['1']['precision'])\n","    recall.append(metrics['1']['recall'])\n","    f_score.append(metrics['1']['f1-score'])\n","    \n","  print('Averaged accuracy:  %.2f' % np.mean(accuracy))\n","  print('Averaged precision: %.2f' % np.mean(precision))\n","  print('Averaged recall:    %.2f' % np.mean(recall))\n","  print('Averaged f_score:   %.2f' % np.mean(f_score))\n","\n","def plot_metrics(history, metric_list=['loss', 'dice_coef'], figsize=(22, 14)):\n","    fig, axes = plt.subplots(len(metric_list), 1, sharex='col', figsize=(22, len(metric_list)*4))\n","    axes = axes.flatten()\n","    \n","    for index, metric in enumerate(metric_list):\n","        axes[index].plot(history[metric], label='Train %s' % metric)\n","        axes[index].plot(history['val_%s' % metric], label='Validation %s' % metric)\n","        axes[index].legend(loc='best')\n","        axes[index].set_title(metric)\n","\n","    plt.xlabel('Epochs')\n","    sns.despine()\n","    plt.show()\n","\n","# Model post process\n","def post_process(probability, threshold=0.5, min_size=10000):\n","    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n","    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n","    predictions = np.zeros(probability.shape, np.float32)\n","    for c in range(1, num_component):\n","        p = (component == c)\n","        if p.sum() > min_size:\n","            predictions[p] = 1\n","    return predictions\n","\n","# Prediction evaluation\n","def get_metrics(model, target_df, df, df_images_dest_path, label_columns, tresholds, min_mask_sizes, N_CLASSES=4, seed=0, preprocessing=None, adjust_fn=None, adjust_param=None, set_name='Complete set', column_names=['Class', 'Dice', 'Dice Post']):\n","    metrics = []\n","\n","    for class_name in label_columns:\n","        metrics.append([class_name, 0, 0])\n","\n","    metrics_df = pd.DataFrame(metrics, columns=column_names)\n","    \n","    for i in range(0, df.shape[0], 500):\n","        batch_idx = list(range(i, min(df.shape[0], i + 500)))\n","        batch_set = df[batch_idx[0]: batch_idx[-1]+1]\n","        ratio = len(batch_set) / len(df)\n","\n","        generator = DataGenerator(\n","                      directory=df_images_dest_path,\n","                      dataframe=batch_set,\n","                      target_df=target_df,\n","                      batch_size=len(batch_set), \n","                      target_size=model.input_shape[1:3],\n","                      n_channels=model.input_shape[3],\n","                      n_classes=N_CLASSES,\n","                      preprocessing=preprocessing,\n","                      adjust_fn=adjust_fn,\n","                      adjust_param=adjust_param,\n","                      seed=seed,\n","                      mode='fit',\n","                      shuffle=False)\n","\n","        x, y = generator.__getitem__(0)\n","        preds = model.predict(x)\n","        \n","        for class_index in range(N_CLASSES):\n","            class_score = []\n","            class_score_post = []\n","            mask_class = y[..., class_index]\n","            pred_class = preds[..., class_index]\n","            for index in range(len(batch_idx)):\n","                sample_mask = mask_class[index, ]\n","                sample_pred = pred_class[index, ]\n","                sample_pred_post = post_process(sample_pred, threshold=tresholds[class_index], min_size=min_mask_sizes[class_index])\n","                if (sample_mask.sum() == 0) & (sample_pred.sum() == 0):\n","                    dice_score = 1.\n","                else:\n","                    dice_score = dice_coefficient(sample_pred, sample_mask)\n","                if (sample_mask.sum() == 0) & (sample_pred_post.sum() == 0):\n","                    dice_score_post = 1.\n","                else:\n","                    dice_score_post = dice_coefficient(sample_pred_post, sample_mask)\n","                class_score.append(dice_score)\n","                class_score_post.append(dice_score_post)\n","            metrics_df.loc[metrics_df[column_names[0]] == label_columns[class_index], column_names[1]] += np.mean(class_score) * ratio\n","            metrics_df.loc[metrics_df[column_names[0]] == label_columns[class_index], column_names[2]] += np.mean(class_score_post) * ratio\n","\n","    metrics_df = metrics_df.append({column_names[0]:set_name, column_names[1]:np.mean(metrics_df[column_names[1]].values), column_names[2]:np.mean(metrics_df[column_names[2]].values)}, ignore_index=True).set_index(column_names[0])\n","    \n","    return metrics_df\n","\n","def get_metrics_ensemble(model_list, target_df, df, df_images_dest_path, label_columns, tresholds, min_mask_sizes, N_CLASSES=4, seed=0, preprocessing=None, adjust_fn=None, adjust_param=None, set_name='Complete set', column_names=['Class', 'Dice', 'Dice Post']):\n","    metrics = []\n","\n","    for class_name in label_columns:\n","        metrics.append([class_name, 0, 0])\n","\n","    metrics_df = pd.DataFrame(metrics, columns=column_names)\n","    \n","    for i in range(0, df.shape[0], 500):\n","        batch_idx = list(range(i, min(df.shape[0], i + 500)))\n","        batch_set = df[batch_idx[0]: batch_idx[-1]+1]\n","        ratio = len(batch_set) / len(df)\n","        \n","        target_size = model_list[0].input_shape[1:3]\n","        n_channels = model_list[0].input_shape[3]\n","\n","        generator = DataGenerator(\n","                      directory=df_images_dest_path,\n","                      dataframe=batch_set,\n","                      target_df=target_df,\n","                      batch_size=len(batch_set), \n","                      target_size=target_size,\n","                      n_channels=n_channels,\n","                      n_classes=N_CLASSES,\n","                      preprocessing=preprocessing,\n","                      adjust_fn=adjust_fn,\n","                      adjust_param=adjust_param,\n","                      seed=seed,\n","                      mode='fit',\n","                      shuffle=False)\n","\n","        x, y = generator.__getitem__(0)\n","        preds = np.zeros((len(batch_set), *target_size, N_CLASSES))\n","        for model in model_list:\n","            preds += model.predict(x)\n","\n","        preds /= len(model_list)\n","        \n","        for class_index in range(N_CLASSES):\n","            class_score = []\n","            class_score_post = []\n","            mask_class = y[..., class_index]\n","            pred_class = preds[..., class_index]\n","            for index in range(len(batch_idx)):\n","                sample_mask = mask_class[index, ]\n","                sample_pred = pred_class[index, ]\n","                sample_pred_post = post_process(sample_pred, threshold=tresholds[class_index], min_size=min_mask_sizes[class_index])\n","                if (sample_mask.sum() == 0) & (sample_pred.sum() == 0):\n","                    dice_score = 1.\n","                else:\n","                    dice_score = dice_coefficient(sample_pred, sample_mask)\n","                if (sample_mask.sum() == 0) & (sample_pred_post.sum() == 0):\n","                    dice_score_post = 1.\n","                else:\n","                    dice_score_post = dice_coefficient(sample_pred_post, sample_mask)\n","                class_score.append(dice_score)\n","                class_score_post.append(dice_score_post)\n","            metrics_df.loc[metrics_df[column_names[0]] == label_columns[class_index], column_names[1]] += np.mean(class_score) * ratio\n","            metrics_df.loc[metrics_df[column_names[0]] == label_columns[class_index], column_names[2]] += np.mean(class_score_post) * ratio\n","\n","    metrics_df = metrics_df.append({column_names[0]:set_name, column_names[1]:np.mean(metrics_df[column_names[1]].values), column_names[2]:np.mean(metrics_df[column_names[2]].values)}, ignore_index=True).set_index(column_names[0])\n","    \n","    return metrics_df\n","\n","def inspect_predictions(df, image_ids, images_dest_path, pred_col=None, label_col='EncodedPixels', title_col='Image_Label', img_shape=(525, 350), figsize=(22, 6)):\n","    if pred_col:\n","        for sample in image_ids:\n","            sample_df = df[df['image'] == sample]\n","            fig, axes = plt.subplots(2, 5, figsize=figsize)\n","            img = cv2.imread(images_dest_path + sample_df['image'].values[0])\n","            img = cv2.resize(img, img_shape)\n","            axes[0][0].imshow(img)\n","            axes[1][0].imshow(img)\n","            axes[0][0].set_title('Label', fontsize=16)\n","            axes[1][0].set_title('Predicted', fontsize=16)\n","            axes[0][0].axis('off')\n","            axes[1][0].axis('off')\n","            for i in range(4):\n","                mask = sample_df[label_col].values[i]\n","                try:\n","                    math.isnan(mask)\n","                    mask = np.zeros((img_shape[1], img_shape[0]))\n","                except:\n","                    mask = rle_decode(mask)\n","                axes[0][i+1].imshow(mask)\n","                axes[1][i+1].imshow(rle2mask(sample_df[pred_col].values[i], img.shape))\n","                axes[0][i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","                axes[1][i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","                axes[0][i+1].axis('off')\n","                axes[1][i+1].axis('off')\n","    else:\n","        for sample in image_ids:\n","            sample_df = df[df['image'] == sample]\n","            fig, axes = plt.subplots(1, 5, figsize=figsize)\n","            img = cv2.imread(images_dest_path + sample_df['image'].values[0])\n","            img = cv2.resize(img, img_shape)\n","            axes[0].imshow(img)\n","            axes[0].set_title('Original', fontsize=16)\n","            axes[0].axis('off')\n","            for i in range(4):\n","                mask = sample_df[label_col].values[i]\n","                try:\n","                    math.isnan(mask)\n","                    mask = np.zeros((img_shape[1], img_shape[0]))\n","                except:\n","                    mask = rle_decode(mask, shape=(img_shape[1], img_shape[0]))\n","                axes[i+1].imshow(mask)\n","                axes[i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","                axes[i+1].axis('off')\n","\n","def inspect_predictions_class(df, image_ids, images_dest_path, pred_col=None, label_col='EncodedPixels', title_col='Image_Label', img_shape=(525, 350), figsize=(22, 6)):\n","  for sample in image_ids:\n","    sample_df = df[df['image'] == sample]\n","    fig, axes = plt.subplots(2, 5, figsize=figsize)\n","    img = cv2.imread(images_dest_path + sample_df['image'].values[0])\n","    img = cv2.resize(img, img_shape)\n","    axes[0][0].imshow(img)\n","    axes[1][0].imshow(img)\n","    axes[0][0].set_title('Label', fontsize=16)\n","    axes[1][0].set_title('Predicted', fontsize=16)\n","    axes[0][0].axis('off')\n","    axes[1][0].axis('off')\n","    for i in range(4):\n","        mask = sample_df[label_col].values[i]\n","        pred_mask = sample_df[pred_col].values[i]\n","        try:\n","            math.isnan(mask)\n","            mask = np.zeros((img_shape[1], img_shape[0]))\n","        except:\n","            mask = rle_decode(mask)            \n","        try:\n","            math.isnan(pred_mask)\n","            pred_mask = np.zeros((img_shape[1], img_shape[0]))\n","        except:\n","            pred_mask = rle_decode(pred_mask)\n","        axes[0][i+1].imshow(mask)\n","        axes[1][i+1].imshow(pred_mask)\n","        axes[0][i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","        axes[1][i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","        axes[0][i+1].axis('off')\n","        axes[1][i+1].axis('off')\n","\n","# Model tunning\n","def classification_tunning(y_true, y_pred, label_columns, threshold_grid=np.arange(0, 1, .01), column_names=['Class', 'Threshold', 'Score'], print_score=True):\n","  metrics = []\n","  for label in label_columns:\n","      for threshold in threshold_grid:\n","          metrics.append([label, threshold, 0])\n","\n","  metrics_df = pd.DataFrame(metrics, columns=column_names)\n","  for index, label in enumerate(label_columns):\n","      for thr in threshold_grid:\n","          metrics_df.loc[(metrics_df[column_names[0]] == label) & (metrics_df[column_names[1]] == thr) , column_names[2]] = fbeta_score(y_true[:,index], (y_pred[:,index] > thr).astype(int), beta=0.25)\n","\n","  best_tresholds = []\n","  best_scores = []\n","  for index, label in enumerate(label_columns):\n","    metrics_df_lbl = metrics_df[metrics_df[column_names[0]] == label_columns[index]]\n","    optimal_values_lbl = metrics_df_lbl.loc[metrics_df_lbl[column_names[2]].idxmax()].values\n","    best_tresholds.append(optimal_values_lbl[1])\n","    best_scores.append(optimal_values_lbl[2])\n","\n","  if print_score:\n","    for index, label in enumerate(label_columns):\n","      print('%s treshold=%.2f Score=%.3f' % (label, best_tresholds[index], best_scores[index]))\n","\n","  return best_tresholds\n","\n","def segmentation_tunning(model, target_df, df, df_images_dest_path, label_columns, mask_grid, threshold_grid=np.arange(0, 1, .01), N_CLASSES=4, preprocessing=None, adjust_fn=None, adjust_param=None, seed=0, column_names=['Class', 'Threshold', 'Mask size', 'Dice'], print_score=True):\n","    metrics = []\n","\n","    for label in label_columns:\n","        for threshold in threshold_grid:\n","            for mask_size in mask_grid:\n","                metrics.append([label, threshold, mask_size, 0])\n","\n","    metrics_df = pd.DataFrame(metrics, columns=column_names)\n","\n","    for i in range(0, df.shape[0], 500):\n","        batch_idx = list(range(i, min(df.shape[0], i + 500)))\n","        batch_set = df[batch_idx[0]: batch_idx[-1]+1]\n","        ratio = len(batch_set) / len(df)\n","\n","        generator = DataGenerator(\n","                      directory=df_images_dest_path,\n","                      dataframe=batch_set,\n","                      target_df=target_df,\n","                      batch_size=len(batch_set), \n","                      target_size=model.input_shape[1:3],\n","                      n_channels=model.input_shape[3],\n","                      n_classes=N_CLASSES,\n","                      preprocessing=preprocessing,\n","                      adjust_fn=adjust_fn,\n","                      adjust_param=adjust_param,\n","                      seed=seed,\n","                      mode='fit',\n","                      shuffle=False)\n","\n","        x, y = generator.__getitem__(0)\n","        preds = model.predict(x)\n","\n","        for class_index, label in enumerate(label_columns):\n","            class_score = []\n","            label_class = y[..., class_index]\n","            pred_class = preds[..., class_index]\n","            for threshold in threshold_grid:\n","                for mask_size in mask_grid:\n","                    mask_score = []\n","                    for index in range(len(batch_idx)):\n","                        label_mask = label_class[index, ]\n","                        pred_mask = pred_class[index, ]\n","                        pred_mask = post_process(pred_mask, threshold=threshold, min_size=mask_size)\n","                        dice_score = dice_coefficient(pred_mask, label_mask)\n","                        if (pred_mask.sum() == 0) & (label_mask.sum() == 0):\n","                            dice_score = 1.\n","                        mask_score.append(dice_score)\n","                    metrics_df.loc[(metrics_df[column_names[0]] == label) & (metrics_df[column_names[1]] == threshold) & \n","                                   (metrics_df[column_names[2]] == mask_size), column_names[3]] += np.mean(mask_score) * ratio\n","                    \n","    best_tresholds = []\n","    best_masks = []\n","    best_dices = []\n","    for index, label in enumerate(label_columns):\n","        metrics_df_lbl = metrics_df[metrics_df[column_names[0]] == label_columns[index]]\n","        optimal_values_lbl = metrics_df_lbl.loc[metrics_df_lbl[column_names[3]].idxmax()].values\n","        best_tresholds.append(optimal_values_lbl[1])\n","        best_masks.append(optimal_values_lbl[2])\n","        best_dices.append(optimal_values_lbl[3])\n","\n","    if print_score:\n","        for index, name in enumerate(label_columns):\n","            print('%s treshold=%.2f mask size=%d Dice=%.3f' % (name, best_tresholds[index], best_masks[index], best_dices[index]))\n","            \n","    return best_tresholds, best_masks\n","\n","# Model utils\n","def ensemble_models(input_shape, model_list, rename_model=False):\n","    if rename_model:\n","        for index, model in enumerate(model_list):\n","            model.name = 'ensemble_' + str(index) + '_' + model.name\n","            for layer in model.layers:\n","                layer.name = 'ensemble_' + str(index) + '_' + layer.name\n","        \n","    inputs = Input(shape=input_shape)\n","    outputs = average([model(inputs) for model in model_list])\n","    \n","    return Model(inputs=inputs, outputs=outputs)\n","\n","# Data generator\n","class DataGenerator(Sequence):\n","    def __init__(self, dataframe, directory, batch_size, n_channels, target_size,  n_classes, \n","                 mode='fit', target_df=None, shuffle=True, preprocessing=None, augmentation=None, adjust_fn=None, adjust_param=None, seed=0):\n","        \n","        self.batch_size = batch_size\n","        self.dataframe = dataframe\n","        self.mode = mode\n","        self.directory = directory\n","        self.target_df = target_df\n","        self.target_size = target_size\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.preprocessing = preprocessing\n","        self.augmentation = augmentation\n","        self.adjust_fn = adjust_fn\n","        self.adjust_param = adjust_param\n","        self.seed = seed\n","        self.mask_shape = (1400, 2100)\n","        self.list_IDs = self.dataframe.index\n","        \n","        if self.seed is not None:\n","            np.random.seed(self.seed)\n","        \n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        return len(self.list_IDs) // self.batch_size\n","\n","    def __getitem__(self, index):\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n","        X = self.__generate_X(list_IDs_batch)\n","        \n","        if self.mode == 'fit':\n","            Y = self.__generate_Y(list_IDs_batch)\n","            \n","            if self.augmentation:\n","                X, Y = self.__augment_batch(X, Y)\n","            \n","            return X, Y\n","        \n","        elif self.mode == 'predict':\n","            return X\n","        \n","    def on_epoch_end(self):\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","    \n","    def __generate_X(self, list_IDs_batch):\n","        X = np.empty((self.batch_size, *self.target_size, self.n_channels))\n","        \n","        for i, ID in enumerate(list_IDs_batch):\n","            img_name = self.dataframe['image'].loc[ID]\n","            img_path = self.directory + img_name\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            \n","            if (not self.adjust_fn is None) & (not self.adjust_param is None):\n","                img = self.adjust_fn(img, self.adjust_param)\n","\n","            if self.preprocessing:\n","                img = self.preprocessing(img)\n","                \n","            X[i,] = img\n","\n","        return X\n","    \n","    def __generate_Y(self, list_IDs_batch):\n","        Y = np.empty((self.batch_size, *self.target_size, self.n_classes), dtype=int)\n","        \n","        for i, ID in enumerate(list_IDs_batch):\n","            img_name = self.dataframe['image'].loc[ID]\n","            image_df = self.target_df[self.target_df['image'] == img_name]\n","            rles = image_df['EncodedPixels'].values\n","            masks = build_masks(rles, input_shape=self.mask_shape, reshape=self.target_size)\n","            Y[i, ] = masks\n","\n","        return Y\n","    \n","    def __augment_batch(self, X_batch, Y_batch):\n","        for i in range(X_batch.shape[0]):\n","            X_batch[i, ], Y_batch[i, ] = self.__random_transform(X_batch[i, ], Y_batch[i, ])\n","        \n","        return X_batch, Y_batch\n","    \n","    def __random_transform(self, X, Y):\n","        composed = self.augmentation(image=X, mask=Y)\n","        X_aug = composed['image']\n","        Y_aug = composed['mask']\n","        \n","        return X_aug, Y_aug"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Segmentation Models: using `keras` framework.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-output":true,"colab_type":"code","id":"ayE1DJg0fRzl","colab":{}},"source":["seed = 0\n","seed_everything(seed)\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C40wYEyOYgu6","colab_type":"code","colab":{}},"source":["base_path = '/content/drive/My Drive/Colab Notebooks/[Kaggle] Understanding Clouds from Satellite Images/'\n","data_path = base_path + 'Data/'\n","classification_model_base_path = base_path + 'Models/files/classification/'\n","classification_model_path = classification_model_base_path + '8-resnet50_224x224.h5'\n","submission_base_path = data_path + 'submissions/inference/'\n","test_path = data_path + 'sample_submission.csv'\n","test_images_path = 'test_images256x384/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6SnKKLczHdqn"},"source":["### Load data"]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","colab_type":"code","id":"pH6kKJKoHdqo","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"d7c2e7af-13f0-47f2-97b7-dd07df13cbb5","executionInfo":{"status":"ok","timestamp":1571964040844,"user_tz":180,"elapsed":1055,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}}},"source":["submission = pd.read_csv(test_path)\n","print('Test samples:', len(submission))\n","\n","# Preprocecss data\n","submission['image'] = submission['Image_Label'].apply(lambda x: x.split('_')[0])\n","test = pd.DataFrame(submission['image'].unique(), columns=['image'])\n","\n","display(test.head())"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>002f507.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0035ae9.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0038327.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>004f759.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>005ba08.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         image\n","0  002f507.jpg\n","1  0035ae9.jpg\n","2  0038327.jpg\n","3  004f759.jpg\n","4  005ba08.jpg"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xyPzEBHGHdqr"},"source":["# Model parameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BBNl0qkSHdqs","colab":{}},"source":["HEIGHT = 224\n","WIDTH = 224\n","CHANNELS = 3\n","N_CLASSES = 4\n","label_columns=['Fish', 'Flower', 'Gravel', 'Sugar']\n","best_tresholds_class = [0.85, 0.79, 0.92, 0.82]\n","\n","model_name = '15-[seg]13-densenet169_[class]8-resnet50_224x224'\n","submission_path = submission_base_path + '%s_submission.csv' % (model_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Idm7ex1GHdq_"},"source":["# Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b-TF9Qn3dVBl","colab":{}},"source":["classification_model = load_model(classification_model_path, custom_objects={'RAdam':RAdam})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kEiavXfkxAzF"},"source":["### Classification data generator"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"colab_type":"code","id":"NYZa5zzHxChz","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"82c694e9-b406-4582-f9d8-47e0077f09a4","executionInfo":{"status":"ok","timestamp":1571964038398,"user_tz":180,"elapsed":1134,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}}},"source":["test_datagen=ImageDataGenerator(rescale=1./255.)\n","\n","classification_test_generator=test_datagen.flow_from_dataframe(\n","                                            dataframe=test,\n","                                            directory=test_images_path,\n","                                            x_col=\"image\",\n","                                            target_size=(HEIGHT, WIDTH),\n","                                            class_mode=None,\n","                                            batch_size=1,\n","                                            shuffle=False,\n","                                            seed=seed)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Found 3698 validated image filenames.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wvsgj0sFrU2i","colab_type":"text"},"source":["# Load predictions"]},{"cell_type":"code","metadata":{"id":"Q1nwxgGbrWvn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"c68085fd-cb65-42a5-bf98-4cdfda883bd6","executionInfo":{"status":"ok","timestamp":1571964165077,"user_tz":180,"elapsed":875,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}}},"source":["prev_submission_path = data_path + 'submissions/segmentation/13-densenet169_submission_post.csv'\n","X_test = pd.read_csv(prev_submission_path)\n","X_test.head()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image_Label</th>\n","      <th>EncodedPixels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>002f507.jpg_Fish</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>002f507.jpg_Flower</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>002f507.jpg_Gravel</td>\n","      <td>3 341 353 341 702 347 1051 59149 60201 349 605...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>002f507.jpg_Sugar</td>\n","      <td>116370 5 116718 8 116739 5 117067 9 117087 9 1...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0035ae9.jpg_Fish</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Image_Label                                      EncodedPixels\n","0    002f507.jpg_Fish                                                NaN\n","1  002f507.jpg_Flower                                                NaN\n","2  002f507.jpg_Gravel  3 341 353 341 702 347 1051 59149 60201 349 605...\n","3   002f507.jpg_Sugar  116370 5 116718 8 116739 5 117067 9 117087 9 1...\n","4    0035ae9.jpg_Fish                                                NaN"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"zors0R3O_V-v","colab_type":"text"},"source":["# Apply classifcation model to test set"]},{"cell_type":"code","metadata":{"id":"lGu30wrM_Xxm","colab_type":"code","colab":{}},"source":["test_class_preds = classification_model.predict_generator(classification_test_generator)\n","\n","for index in range(len(label_columns)):\n","  test_class_preds[:,index] = (test_class_preds[:,index] > best_tresholds_class[index]).astype(int)\n","  \n","X_test['has_mask'] = test_class_preds.reshape(test_class_preds.shape[0]*N_CLASSES)\n","X_test['EncodedPixels_pred'] = X_test.apply(lambda row: row['EncodedPixels'] if row['has_mask'] == 1 else np.nan, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gc7F9fRkwOup","colab_type":"text"},"source":["### Submission with mask classification"]},{"cell_type":"code","metadata":{"id":"ybZkgbhtwRs4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"80b43c88-3555-44cf-adab-a68b2258f03e","executionInfo":{"status":"ok","timestamp":1571964992441,"user_tz":180,"elapsed":1579,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}}},"source":["submission_df = X_test[['Image_Label' ,'EncodedPixels_pred']]\n","submission_df.columns = ['Image_Label' ,'EncodedPixels']\n","submission_df.to_csv(submission_path, index=False)\n","display(submission_df.head())"],"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image_Label</th>\n","      <th>EncodedPixels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>002f507.jpg_Fish</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>002f507.jpg_Flower</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>002f507.jpg_Gravel</td>\n","      <td>3 341 353 341 702 347 1051 59149 60201 349 605...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>002f507.jpg_Sugar</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0035ae9.jpg_Fish</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Image_Label                                      EncodedPixels\n","0    002f507.jpg_Fish                                                NaN\n","1  002f507.jpg_Flower                                                NaN\n","2  002f507.jpg_Gravel  3 341 353 341 702 347 1051 59149 60201 349 605...\n","3   002f507.jpg_Sugar                                                NaN\n","4    0035ae9.jpg_Fish                                                NaN"]},"metadata":{"tags":[]}}]}]}
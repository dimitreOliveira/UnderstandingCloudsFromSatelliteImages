{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ML Virtus\\AppData\\Roaming\\Python\\Python36\\site-packages\\classification_models\\resnext\\__init__.py:4: UserWarning: Current ResNext models are deprecated, use keras.applications ResNeXt models\n",
      "  warnings.warn('Current ResNext models are deprecated, '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import albumentations as albu\n",
    "from keras_radam import RAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models as sm\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import Sequence, multi_gpu_model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "seed = 0\n",
    "seed_everything(seed)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_path = '../data/train.csv'\n",
    "submission_set_path = '../data/sample_submission.csv'\n",
    "hold_out_set_path = '../data/hold-out.csv'\n",
    "\n",
    "HEIGHT = 256\n",
    "WIDTH = 384\n",
    "train_base_path = '../data/train_images/'\n",
    "test_base_path = '../data/test_images/'\n",
    "train_images_dest_path = '../data/train_images%sx%s/' % (HEIGHT, WIDTH)\n",
    "validation_images_dest_path = '../data/validation_images%sx%s/' % (HEIGHT, WIDTH)\n",
    "test_images_dest_path = '../data/test_images%sx%s/' % (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compete set samples: 22184\n",
      "Train samples:  4420\n",
      "Validation samples:  1105\n",
      "Test samples: 14792\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>Fish_mask</th>\n",
       "      <th>Flower_mask</th>\n",
       "      <th>Gravel_mask</th>\n",
       "      <th>Sugar_mask</th>\n",
       "      <th>Fish</th>\n",
       "      <th>Flower</th>\n",
       "      <th>Gravel</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>697eb53.jpg</td>\n",
       "      <td>52180 1006 53580 1006 54980 1006 56380 1006 57...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7fdc6fa.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142270 496 143670 496 145070 496 146470 496 14...</td>\n",
       "      <td>1054319 357 1055719 348 1056070 3 1056078 11 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b977433.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2444526 1040 2445926 1040 2447326 1040 2448726...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15415 1057 16815 1057 18215 1057 19615 1057 21...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9159a72.jpg</td>\n",
       "      <td>290317 412 291717 412 293117 412 294517 412 29...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2096523 256 2097923 256 2099323 256 2100723 25...</td>\n",
       "      <td>830 258 2230 258 3630 258 5030 258 6430 258 78...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>676806e.jpg</td>\n",
       "      <td>12640 848 14040 848 15440 848 16840 848 18240 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>575687 502 577087 502 578487 502 579887 502 58...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image                                          Fish_mask  \\\n",
       "0  697eb53.jpg  52180 1006 53580 1006 54980 1006 56380 1006 57...   \n",
       "1  7fdc6fa.jpg                                                NaN   \n",
       "2  b977433.jpg                                                NaN   \n",
       "3  9159a72.jpg  290317 412 291717 412 293117 412 294517 412 29...   \n",
       "4  676806e.jpg  12640 848 14040 848 15440 848 16840 848 18240 ...   \n",
       "\n",
       "                                         Flower_mask  \\\n",
       "0                                                NaN   \n",
       "1  142270 496 143670 496 145070 496 146470 496 14...   \n",
       "2  2444526 1040 2445926 1040 2447326 1040 2448726...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         Gravel_mask  \\\n",
       "0                                                NaN   \n",
       "1  1054319 357 1055719 348 1056070 3 1056078 11 1...   \n",
       "2                                                NaN   \n",
       "3  2096523 256 2097923 256 2099323 256 2100723 25...   \n",
       "4  575687 502 577087 502 578487 502 579887 502 58...   \n",
       "\n",
       "                                          Sugar_mask  Fish  Flower  Gravel  \\\n",
       "0                                                NaN     1       0       0   \n",
       "1                                                NaN     0       1       1   \n",
       "2  15415 1057 16815 1057 18215 1057 19615 1057 21...     0       1       0   \n",
       "3  830 258 2230 258 3630 258 5030 258 6430 258 78...     1       0       1   \n",
       "4                                                NaN     1       0       1   \n",
       "\n",
       "   Sugar    set  \n",
       "0      0  train  \n",
       "1      0  train  \n",
       "2      1  train  \n",
       "3      1  train  \n",
       "4      0  train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(train_set_path)\n",
    "submission = pd.read_csv(submission_set_path)\n",
    "\n",
    "hold_out_set = pd.read_csv(hold_out_set_path)\n",
    "X_train = hold_out_set[hold_out_set['set'] == 'train']\n",
    "X_val = hold_out_set[hold_out_set['set'] == 'validation']\n",
    "\n",
    "print('Compete set samples:', len(train))\n",
    "print('Train samples: ', len(X_train))\n",
    "print('Validation samples: ', len(X_val))\n",
    "print('Test samples:', len(submission))\n",
    "\n",
    "# Preprocecss data\n",
    "train['image'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "submission['image'] = submission['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "test = pd.DataFrame(submission['image'].unique(), columns=['image'])\n",
    "test['set'] = 'test'\n",
    "\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GPUS = 3\n",
    "BACKBONE = 'resnext50'\n",
    "BATCH_SIZE = 16 * N_GPUS\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 3e-4\n",
    "HEIGHT = 256\n",
    "WIDTH = 384\n",
    "CHANNELS = 3\n",
    "N_CLASSES = 4\n",
    "ES_PATIENCE = 5\n",
    "RLROP_PATIENCE = 3\n",
    "DECAY_DROP = 0.5\n",
    "model_path = '../models/bins/4_uNet_%s_%sx%s.h5' % (BACKBONE, HEIGHT, WIDTH)\n",
    "\n",
    "submission_path = '../submissions/uNet_%s_%sx%s_submission.csv' % (BACKBONE, HEIGHT, WIDTH)\n",
    "submission_post_path = '../submissions/uNet_%s_%sx%s_submission_post.csv' % (BACKBONE, HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "preprocessing = sm.backbones.get_preprocessing(BACKBONE)\n",
    "\n",
    "augmentation = albu.Compose([albu.HorizontalFlip(p=0.5),\n",
    "                             albu.VerticalFlip(p=0.5),\n",
    "                             albu.GridDistortion(p=0.5),\n",
    "                             albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, \n",
    "                                                   shift_limit=0.1, border_mode=0, p=0.5),\n",
    "                             albu.OpticalDistortion(p=0.5, distort_limit=2, shift_limit=0.5)\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def np_resize(img, input_shape):\n",
    "    height, width = input_shape\n",
    "    return cv2.resize(img, (width, height))\n",
    "    \n",
    "def mask2rle(img):\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def build_rles(masks, reshape=None):\n",
    "    width, height, depth = masks.shape\n",
    "    rles = []\n",
    "    \n",
    "    for i in range(depth):\n",
    "        mask = masks[:, :, i]\n",
    "        \n",
    "        if reshape:\n",
    "            mask = mask.astype(np.float32)\n",
    "            mask = np_resize(mask, reshape).astype(np.int64)\n",
    "        \n",
    "        rle = mask2rle(mask)\n",
    "        rles.append(rle)\n",
    "        \n",
    "    return rles\n",
    "\n",
    "def build_masks(rles, input_shape, reshape=None):\n",
    "    depth = len(rles)\n",
    "    if reshape is None:\n",
    "        masks = np.zeros((*input_shape, depth))\n",
    "    else:\n",
    "        masks = np.zeros((*reshape, depth))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            if reshape is None:\n",
    "                masks[:, :, i] = rle2mask(rle, input_shape)\n",
    "            else:\n",
    "                mask = rle2mask(rle, input_shape)\n",
    "                reshaped_mask = np_resize(mask, reshape)\n",
    "                masks[:, :, i] = reshaped_mask\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def rle2mask(rle, input_shape):\n",
    "    width, height = input_shape[:2]\n",
    "    mask = np.zeros( width*height ).astype(np.uint8)\n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        current_position += lengths[index]\n",
    "        \n",
    "    return mask.reshape(height, width).T\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).astype(np.bool)\n",
    "    y_pred = np.asarray(y_pred).astype(np.bool)\n",
    "    intersection = np.logical_and(y_true, y_pred)\n",
    "    return (2. * intersection.sum()) / (y_true.sum() + y_pred.sum())\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def post_process(probability, threshold=0.5, min_size=10000):\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros(probability.shape, np.float32)\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "    return predictions\n",
    "            \n",
    "def get_metrics(model, df, df_images_dest_path, tresholds, min_mask_sizes, set_name='Complete set'):\n",
    "    class_names = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "    metrics = []\n",
    "\n",
    "    for class_name in class_names:\n",
    "        metrics.append([class_name, 0, 0])\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics, columns=['Class', 'Dice', 'Dice Post'])\n",
    "    \n",
    "    for i in range(0, df.shape[0], 500):\n",
    "        batch_idx = list(range(i, min(df.shape[0], i + 500)))\n",
    "        batch_set = df[batch_idx[0]: batch_idx[-1]+1]\n",
    "        ratio = len(batch_set) / len(df)\n",
    "\n",
    "        generator = DataGenerator(\n",
    "                      directory=df_images_dest_path,\n",
    "                      dataframe=batch_set,\n",
    "                      target_df=train,\n",
    "                      batch_size=len(batch_set), \n",
    "                      target_size=(HEIGHT, WIDTH),\n",
    "                      n_channels=CHANNELS,\n",
    "                      n_classes=N_CLASSES,\n",
    "                      preprocessing=preprocessing,\n",
    "                      seed=seed,\n",
    "                      mode='fit',\n",
    "                      shuffle=False)\n",
    "\n",
    "        x, y = generator.__getitem__(0)\n",
    "        preds = model.predict(x)\n",
    "        \n",
    "        for class_index in range(N_CLASSES):\n",
    "            class_score = []\n",
    "            class_score_post = []\n",
    "            mask_class = y[..., class_index]\n",
    "            pred_class = preds[..., class_index]\n",
    "            for index in range(len(batch_idx)):\n",
    "                sample_mask = mask_class[index, ]\n",
    "                sample_pred = pred_class[index, ]\n",
    "                sample_pred_post = post_process(sample_pred, threshold=tresholds[class_index], min_size=min_mask_sizes[class_index])\n",
    "                if (sample_mask.sum() == 0) & (sample_pred.sum() == 0):\n",
    "                    dice_score = 1.\n",
    "                else:\n",
    "                    dice_score = dice_coefficient(sample_pred, sample_mask)\n",
    "                if (sample_mask.sum() == 0) & (sample_pred_post.sum() == 0):\n",
    "                    dice_score_post = 1.\n",
    "                else:\n",
    "                    dice_score_post = dice_coefficient(sample_pred_post, sample_mask)\n",
    "                class_score.append(dice_score)\n",
    "                class_score_post.append(dice_score_post)\n",
    "            metrics_df.loc[metrics_df['Class'] == class_names[class_index], 'Dice'] += np.mean(class_score) * ratio\n",
    "            metrics_df.loc[metrics_df['Class'] == class_names[class_index], 'Dice Post'] += np.mean(class_score_post) * ratio\n",
    "\n",
    "    metrics_df = metrics_df.append({'Class':set_name, 'Dice':np.mean(metrics_df['Dice'].values), 'Dice Post':np.mean(metrics_df['Dice Post'].values)}, ignore_index=True).set_index('Class')\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "def plot_metrics(history):\n",
    "    fig, axes = plt.subplots(4, 1, sharex='col', figsize=(22, 14))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    axes[0].plot(history['loss'], label='Train loss')\n",
    "    axes[0].plot(history['val_loss'], label='Validation loss')\n",
    "    axes[0].legend(loc='best')\n",
    "    axes[0].set_title('Loss')\n",
    "\n",
    "    axes[1].plot(history['iou_score'], label='Train IOU Score')\n",
    "    axes[1].plot(history['val_iou_score'], label='Validation IOU Score')\n",
    "    axes[1].legend(loc='best')\n",
    "    axes[1].set_title('IOU Score')\n",
    "\n",
    "    axes[2].plot(history['dice_coef'], label='Train Dice coefficient')\n",
    "    axes[2].plot(history['val_dice_coef'], label='Validation Dice coefficient')\n",
    "    axes[2].legend(loc='best')\n",
    "    axes[2].set_title('Dice coefficient')\n",
    "\n",
    "    axes[3].plot(history['score'], label='Train F-Score')\n",
    "    axes[3].plot(history['val_score'], label='Validation F-Score')\n",
    "    axes[3].legend(loc='best')\n",
    "    axes[3].set_title('F-Score')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "    \n",
    "def cosine_decay_with_warmup(global_step,\n",
    "                             learning_rate_base,\n",
    "                             total_steps,\n",
    "                             warmup_learning_rate=0.0,\n",
    "                             warmup_steps=0,\n",
    "                             hold_base_rate_steps=0):\n",
    "    \"\"\"\n",
    "    Cosine decay schedule with warm up period.\n",
    "    In this schedule, the learning rate grows linearly from warmup_learning_rate\n",
    "    to learning_rate_base for warmup_steps, then transitions to a cosine decay\n",
    "    schedule.\n",
    "    :param global_step {int}: global step.\n",
    "    :param learning_rate_base {float}: base learning rate.\n",
    "    :param total_steps {int}: total number of training steps.\n",
    "    :param warmup_learning_rate {float}: initial learning rate for warm up. (default: {0.0}).\n",
    "    :param warmup_steps {int}: number of warmup steps. (default: {0}).\n",
    "    :param hold_base_rate_steps {int}: Optional number of steps to hold base learning rate before decaying. (default: {0}).\n",
    "    :param global_step {int}: global step.\n",
    "    :Returns : a float representing learning rate.\n",
    "    :Raises ValueError: if warmup_learning_rate is larger than learning_rate_base, or if warmup_steps is larger than total_steps.\n",
    "    \"\"\"\n",
    "\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to warmup_steps.')\n",
    "    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n",
    "        np.pi *\n",
    "        (global_step - warmup_steps - hold_base_rate_steps\n",
    "         ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "\n",
    "class WarmUpCosineDecayScheduler(Callback):\n",
    "    \"\"\"Cosine decay with warmup learning rate scheduler\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate_base,\n",
    "                 total_steps,\n",
    "                 global_step_init=0,\n",
    "                 warmup_learning_rate=0.0,\n",
    "                 warmup_steps=0,\n",
    "                 hold_base_rate_steps=0,\n",
    "                 verbose=0):\n",
    "        \"\"\"\n",
    "        Constructor for cosine decay with warmup learning rate scheduler.\n",
    "        :param learning_rate_base {float}: base learning rate.\n",
    "        :param total_steps {int}: total number of training steps.\n",
    "        :param global_step_init {int}: initial global step, e.g. from previous checkpoint.\n",
    "        :param warmup_learning_rate {float}: initial learning rate for warm up. (default: {0.0}).\n",
    "        :param warmup_steps {int}: number of warmup steps. (default: {0}).\n",
    "        :param hold_base_rate_steps {int}: Optional number of steps to hold base learning rate before decaying. (default: {0}).\n",
    "        :param verbose {int}: quiet, 1: update messages. (default: {0}).\n",
    "        \"\"\"\n",
    "\n",
    "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = cosine_decay_with_warmup(global_step=self.global_step,\n",
    "                                      learning_rate_base=self.learning_rate_base,\n",
    "                                      total_steps=self.total_steps,\n",
    "                                      warmup_learning_rate=self.warmup_learning_rate,\n",
    "                                      warmup_steps=self.warmup_steps,\n",
    "                                      hold_base_rate_steps=self.hold_base_rate_steps)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nBatch %02d: setting learning rate to %s.' % (self.global_step + 1, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, dataframe, target_df=None, mode='fit', directory=train_images_dest_path,\n",
    "                 batch_size=BATCH_SIZE, n_channels=CHANNELS, target_size=(HEIGHT, WIDTH), \n",
    "                 n_classes=N_CLASSES, seed=seed, shuffle=True, preprocessing=None, augmentation=None):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dataframe = dataframe\n",
    "        self.mode = mode\n",
    "        self.directory = directory\n",
    "        self.target_df = target_df\n",
    "        self.target_size = target_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.seed = seed\n",
    "        self.mask_shape = (1400, 2100)\n",
    "        self.list_IDs = self.dataframe.index\n",
    "        \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "        X = self.__generate_X(list_IDs_batch)\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "            Y = self.__generate_Y(list_IDs_batch)\n",
    "            \n",
    "            if self.augmentation:\n",
    "                X, Y = self.__augment_batch(X, Y)\n",
    "            \n",
    "            return X, Y\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __generate_X(self, list_IDs_batch):\n",
    "        X = np.empty((self.batch_size, *self.target_size, self.n_channels))\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            img_name = self.dataframe['image'].loc[ID]\n",
    "            img_path = self.directory + img_name\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.preprocessing:\n",
    "                img = self.preprocessing(img)\n",
    "                \n",
    "            X[i,] = img\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __generate_Y(self, list_IDs_batch):\n",
    "        Y = np.empty((self.batch_size, *self.target_size, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            img_name = self.dataframe['image'].loc[ID]\n",
    "            image_df = self.target_df[self.target_df['image'] == img_name]\n",
    "            rles = image_df['EncodedPixels'].values\n",
    "            masks = build_masks(rles, input_shape=self.mask_shape, reshape=self.target_size)\n",
    "            Y[i, ] = masks\n",
    "\n",
    "        return Y\n",
    "    \n",
    "    def __augment_batch(self, X_batch, Y_batch):\n",
    "        for i in range(X_batch.shape[0]):\n",
    "            X_batch[i, ], Y_batch[i, ] = self.__random_transform(X_batch[i, ], Y_batch[i, ])\n",
    "        \n",
    "        return X_batch, Y_batch\n",
    "    \n",
    "    def __random_transform(self, X, Y):\n",
    "        composed = self.augmentation(image=X, mask=Y)\n",
    "        X_aug = composed['image']\n",
    "        Y_aug = composed['mask']\n",
    "        \n",
    "        return X_aug, Y_aug\n",
    "    \n",
    "train_generator = DataGenerator(\n",
    "                  directory=train_images_dest_path,\n",
    "                  dataframe=X_train,\n",
    "                  target_df=train,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  target_size=(HEIGHT, WIDTH),\n",
    "                  n_channels=CHANNELS,\n",
    "                  n_classes=N_CLASSES,\n",
    "                  preprocessing=preprocessing,\n",
    "                  augmentation=augmentation,\n",
    "                  seed=seed)\n",
    "\n",
    "valid_generator = DataGenerator(\n",
    "                  directory=validation_images_dest_path,\n",
    "                  dataframe=X_val,\n",
    "                  target_df=train,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  target_size=(HEIGHT, WIDTH),\n",
    "                  n_channels=CHANNELS,\n",
    "                  n_classes=N_CLASSES,\n",
    "                  preprocessing=preprocessing,\n",
    "                  seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               (None, 256, 384, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256, 384, 3)  0           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 256, 384, 3)  0           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 256, 384, 3)  0           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "u-resnext50 (Model)             (None, 256, 384, 4)  32063629    lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sigmoid (Concatenate)           (None, 256, 384, 4)  0           u-resnext50[1][0]                \n",
      "                                                                 u-resnext50[2][0]                \n",
      "                                                                 u-resnext50[3][0]                \n",
      "==================================================================================================\n",
      "Total params: 32,063,629\n",
      "Trainable params: 31,993,415\n",
      "Non-trainable params: 70,214\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name=BACKBONE, \n",
    "                encoder_weights='imagenet',\n",
    "                classes=N_CLASSES,\n",
    "                activation='sigmoid',\n",
    "                input_shape=(HEIGHT, WIDTH, CHANNELS))\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)\n",
    "\n",
    "metric_list = [dice_coef, sm.metrics.iou_score, sm.metrics.f1_score]\n",
    "callback_list = [checkpoint, es, rlrop]\n",
    "optimizer = RAdam(learning_rate=LEARNING_RATE, warmup_proportion=0.2)\n",
    "\n",
    "parallel_model = multi_gpu_model(model, gpus=N_GPUS)\n",
    "parallel_model.compile(optimizer=optimizer, loss=sm.losses.bce_dice_loss, metrics=metric_list)\n",
    "\n",
    "parallel_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "41/92 [============>.................] - ETA: 8:16 - loss: 1.5615 - dice_coef: 0.2409 - iou_score: 0.1129 - score: 0.1781"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN = len(X_train)//BATCH_SIZE\n",
    "STEP_SIZE_VALID = len(X_val)//BATCH_SIZE\n",
    "\n",
    "history = parallel_model.fit_generator(generator=train_generator,\n",
    "                                       steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                                       validation_data=valid_generator,\n",
    "                                       validation_steps=STEP_SIZE_VALID,\n",
    "                                       callbacks=callback_list,\n",
    "                                       epochs=EPOCHS,\n",
    "                                       verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold and mask size tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class_names = ['Fish  ', 'Flower', 'Gravel', 'Sugar ']\n",
    "mask_grid = [0, 500, 1000, 5000, 7500, 10000, 15000]\n",
    "threshold_grid = np.arange(.5, 1, .05)\n",
    "metrics = []\n",
    "\n",
    "for class_index in range(N_CLASSES):\n",
    "    for threshold in threshold_grid:\n",
    "        for mask_size in mask_grid:\n",
    "            metrics.append([class_index, threshold, mask_size, 0])\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Class', 'Threshold', 'Mask size', 'Dice'])\n",
    "\n",
    "for i in range(0, X_val.shape[0], 500):\n",
    "    batch_idx = list(range(i, min(X_val.shape[0], i + 500)))\n",
    "    batch_set = X_val[batch_idx[0]: batch_idx[-1]+1]\n",
    "    ratio = len(batch_set) / len(X_val)\n",
    "    \n",
    "    generator = DataGenerator(\n",
    "                  directory=validation_images_dest_path,\n",
    "                  dataframe=batch_set,\n",
    "                  target_df=train,\n",
    "                  batch_size=len(batch_set), \n",
    "                  target_size=(HEIGHT, WIDTH),\n",
    "                  n_channels=CHANNELS,\n",
    "                  n_classes=N_CLASSES,\n",
    "                  preprocessing=preprocessing,\n",
    "                  seed=seed,\n",
    "                  mode='fit',\n",
    "                  shuffle=False)\n",
    "    \n",
    "    x, y = generator.__getitem__(0)\n",
    "    preds = parallel_model.predict(x)\n",
    "\n",
    "    for class_index in range(N_CLASSES):\n",
    "        class_score = []\n",
    "        label_class = y[..., class_index]\n",
    "        pred_class = preds[..., class_index]\n",
    "        for threshold in threshold_grid:\n",
    "            for mask_size in mask_grid:\n",
    "                mask_score = []\n",
    "                for index in range(len(batch_idx)):\n",
    "                    label_mask = label_class[index, ]\n",
    "                    pred_mask = pred_class[index, ]\n",
    "                    pred_mask = post_process(pred_mask, threshold=threshold, min_size=mask_size)\n",
    "                    dice_score = dice_coefficient(pred_mask, label_mask)\n",
    "                    if (pred_mask.sum() == 0) & (label_mask.sum() == 0):\n",
    "                        dice_score = 1.\n",
    "                    mask_score.append(dice_score)\n",
    "                metrics_df.loc[(metrics_df['Class'] == class_index) & (metrics_df['Threshold'] == threshold) & \n",
    "                               (metrics_df['Mask size'] == mask_size), 'Dice'] += np.mean(mask_score) * ratio\n",
    "\n",
    "metrics_df_0 = metrics_df[metrics_df['Class'] == 0]\n",
    "metrics_df_1 = metrics_df[metrics_df['Class'] == 1]\n",
    "metrics_df_2 = metrics_df[metrics_df['Class'] == 2]\n",
    "metrics_df_3 = metrics_df[metrics_df['Class'] == 3]\n",
    "\n",
    "optimal_values_0 = metrics_df_0.loc[metrics_df_0['Dice'].idxmax()].values\n",
    "optimal_values_1 = metrics_df_1.loc[metrics_df_1['Dice'].idxmax()].values\n",
    "optimal_values_2 = metrics_df_2.loc[metrics_df_2['Dice'].idxmax()].values\n",
    "optimal_values_3 = metrics_df_3.loc[metrics_df_3['Dice'].idxmax()].values\n",
    "\n",
    "best_tresholds = [optimal_values_0[1], optimal_values_1[1], optimal_values_2[1], optimal_values_3[1]]\n",
    "best_masks = [optimal_values_0[2], optimal_values_1[2], optimal_values_2[2], optimal_values_3[2]]\n",
    "best_dices = [optimal_values_0[3], optimal_values_1[3], optimal_values_2[3], optimal_values_3[3]]\n",
    "\n",
    "for index, name in enumerate(class_names):\n",
    "    print('%s treshold=%.2f mask size=%d Dice=%.3f' % (name, best_tresholds[index], best_masks[index], best_dices[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_metrics = get_metrics(parallel_model, X_train, train_images_dest_path, best_tresholds, best_masks, 'Train')\n",
    "display(train_metrics)\n",
    "\n",
    "validation_metrics = get_metrics(parallel_model, X_val, validation_images_dest_path, best_tresholds, best_masks, 'Validation')\n",
    "display(validation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply model to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "test_df = []\n",
    "\n",
    "for i in range(0, test.shape[0], 500):\n",
    "    batch_idx = list(range(i, min(test.shape[0], i + 500)))\n",
    "    batch_set = test[batch_idx[0]: batch_idx[-1]+1]\n",
    "    \n",
    "    test_generator = DataGenerator(\n",
    "                      directory=test_images_dest_path,\n",
    "                      dataframe=batch_set,\n",
    "                      target_df=submission,\n",
    "                      batch_size=1, \n",
    "                      target_size=(HEIGHT, WIDTH),\n",
    "                      n_channels=CHANNELS,\n",
    "                      n_classes=N_CLASSES,\n",
    "                      preprocessing=preprocessing,\n",
    "                      seed=seed,\n",
    "                      mode='predict',\n",
    "                      shuffle=False)\n",
    "    \n",
    "    preds = parallel_model.predict_generator(test_generator)\n",
    "\n",
    "    for index, b in enumerate(batch_idx):\n",
    "        filename = test['image'].iloc[b]\n",
    "        image_df = submission[submission['image'] == filename].copy()\n",
    "        pred_masks = preds[index, ].round().astype(int)\n",
    "        pred_rles = build_rles(pred_masks, reshape=(350, 525))\n",
    "        image_df['EncodedPixels'] = pred_rles\n",
    "\n",
    "        ### Post procecssing\n",
    "        pred_masks_post = preds[index, ].astype('float32') \n",
    "        for class_index in range(N_CLASSES):\n",
    "            pred_mask = pred_masks_post[...,class_index]\n",
    "            pred_mask = post_process(pred_mask, threshold=best_tresholds[class_index], min_size=best_masks[class_index])\n",
    "            pred_masks_post[...,class_index] = pred_mask\n",
    "\n",
    "        pred_rles_post = build_rles(pred_masks_post, reshape=(350, 525))\n",
    "        image_df['EncodedPixels_post'] = pred_rles_post\n",
    "        ###\n",
    "        \n",
    "        test_df.append(image_df)\n",
    "\n",
    "sub_df = pd.concat(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "submission_df = sub_df[['Image_Label' ,'EncodedPixels']]\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission with post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "submission_df_post = sub_df[['Image_Label' ,'EncodedPixels_post']]\n",
    "submission_df_post.columns = ['Image_Label' ,'EncodedPixels']\n",
    "submission_df_post.to_csv(submission_post_path, index=False)\n",
    "display(submission_df_post.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

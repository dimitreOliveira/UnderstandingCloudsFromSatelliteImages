{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"50-[K-Fold5]unet-densenet169_384x480.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ahe394ceHqxV","colab_type":"code","outputId":"87abb630-b402-4eca-fee9-6eafa24b6a5a","executionInfo":{"status":"ok","timestamp":1571338457773,"user_tz":180,"elapsed":19731,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yxvzFnySHdqd","colab_type":"text"},"source":["### Dependencies"]},{"cell_type":"code","metadata":{"id":"E2nPuJEq5u9S","colab_type":"code","colab":{}},"source":["!unzip -q '/content/drive/My Drive/Colab Notebooks/[Kaggle] Understanding Clouds from Satellite Images/Data/train_images384x480.zip'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"cG_j8ky3Hdqk","colab_type":"code","cellView":"form","outputId":"e55337e2-7183-47f6-e50a-32bc0260bd8c","executionInfo":{"status":"ok","timestamp":1571338484549,"user_tz":180,"elapsed":46493,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["#@title\n","# Dependencies\n","import os\n","import cv2\n","import math\n","import random\n","import shutil\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from skimage import exposure\n","import multiprocessing as mp\n","import albumentations as albu\n","import matplotlib.pyplot as plt\n","from tensorflow import set_random_seed\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from keras import backend as K\n","from keras.utils import Sequence\n","from keras import optimizers, applications\n","from keras.models import Model, load_model\n","from keras.losses import binary_crossentropy\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Dense, GlobalAveragePooling2D, Input\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","# Required repositories\n","os.system('pip install segmentation-models')\n","os.system('pip install keras-rectified-adam')\n","os.system('pip install tta-wrapper')\n","\n","from keras_radam import RAdam\n","import segmentation_models as sm\n","from tta_wrapper import tta_segmentation\n","\n","# Misc\n","def seed_everything(seed=0):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    set_random_seed(seed)\n","    \n","    \n","# Segmentation related\n","def rle_decode(mask_rle, shape=(1400, 2100)):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape, order='F')  # Needed to align to RLE direction\n","\n","def rle_to_mask(rle_string, height, width):\n","    rows, cols = height, width\n","    \n","    if rle_string == -1:\n","        return np.zeros((height, width))\n","    else:\n","        rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n","        rle_pairs = np.array(rle_numbers).reshape(-1,2)\n","        img = np.zeros(rows*cols, dtype=np.uint8)\n","        for index, length in rle_pairs:\n","            index -= 1\n","            img[index:index+length] = 255\n","        img = img.reshape(cols,rows)\n","        img = img.T\n","        return img\n","    \n","def get_mask_area(df, index, column_name, shape=(1400, 2100)):\n","    rle = df.loc[index][column_name]\n","    try:\n","        math.isnan(rle)\n","        np_mask = np.zeros((shape[0], shape[1], 3))\n","    except:\n","        np_mask = rle_to_mask(rle, shape[0], shape[1])\n","        np_mask = np.clip(np_mask, 0, 1)\n","        \n","    return int(np.sum(np_mask))\n","\n","def np_resize(img, input_shape):\n","    \"\"\"\n","    Reshape a numpy array, which is input_shape=(height, width), \n","    as opposed to input_shape=(width, height) for cv2\n","    \"\"\"\n","    height, width = input_shape\n","    return cv2.resize(img, (width, height))\n","    \n","def mask2rle(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels= img.T.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n","\n","def build_rles(masks, reshape=None):\n","    width, height, depth = masks.shape\n","    rles = []\n","    \n","    for i in range(depth):\n","        mask = masks[:, :, i]\n","        \n","        if reshape:\n","            mask = mask.astype(np.float32)\n","            mask = np_resize(mask, reshape).astype(np.int64)\n","        \n","        rle = mask2rle(mask)\n","        rles.append(rle)\n","        \n","    return rles\n","\n","def build_masks(rles, input_shape, reshape=None):\n","    depth = len(rles)\n","    if reshape is None:\n","        masks = np.zeros((*input_shape, depth))\n","    else:\n","        masks = np.zeros((*reshape, depth))\n","    \n","    for i, rle in enumerate(rles):\n","        if type(rle) is str:\n","            if reshape is None:\n","                masks[:, :, i] = rle2mask(rle, input_shape)\n","            else:\n","                mask = rle2mask(rle, input_shape)\n","                reshaped_mask = np_resize(mask, reshape)\n","                masks[:, :, i] = reshaped_mask\n","    \n","    return masks\n","\n","def rle2mask(rle, input_shape):\n","    width, height = input_shape[:2]\n","    mask = np.zeros( width*height ).astype(np.uint8)\n","    array = np.asarray([int(x) for x in rle.split()])\n","    starts = array[0::2]\n","    lengths = array[1::2]\n","\n","    current_position = 0\n","    for index, start in enumerate(starts):\n","        mask[int(start):int(start+lengths[index])] = 1\n","        current_position += lengths[index]\n","        \n","    return mask.reshape(height, width).T\n","\n","def dice_coefficient(y_true, y_pred):\n","    y_true = np.asarray(y_true).astype(np.bool)\n","    y_pred = np.asarray(y_pred).astype(np.bool)\n","    intersection = np.logical_and(y_true, y_pred)\n","    return (2. * intersection.sum()) / (y_true.sum() + y_pred.sum())\n","\n","def dice_coef(y_true, y_pred, smooth=1):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","    \n","# Data pre-process\n","def preprocess_image(image_id, base_path, save_path, HEIGHT, WIDTH):\n","    image = cv2.imread(base_path + image_id)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = cv2.resize(image, (WIDTH, HEIGHT))\n","    cv2.imwrite(save_path + image_id, image)\n","    \n","def pre_process_set(df, preprocess_fn):\n","    n_cpu = mp.cpu_count()\n","    df_n_cnt = df.shape[0]//n_cpu\n","    pool = mp.Pool(n_cpu)\n","    \n","    dfs = [df.iloc[df_n_cnt*i:df_n_cnt*(i+1)] for i in range(n_cpu)]\n","    dfs[-1] = df.iloc[df_n_cnt*(n_cpu-1):]\n","    res = pool.map(preprocess_fn, [x_df for x_df in dfs])\n","    pool.close()\n","        \n","# def preprocess_data(df, HEIGHT=HEIGHT, WIDTH=WIDTH):\n","#     df = df.reset_index()\n","#     for i in range(df.shape[0]):\n","#         item = df.iloc[i]\n","#         image_id = item['image']\n","#         item_set = item['set']\n","#         if item_set == 'train':\n","#             preprocess_image(image_id, train_base_path, train_images_dest_path, HEIGHT, WIDTH)\n","#         if item_set == 'validation':\n","#             preprocess_image(image_id, train_base_path, validation_images_dest_path, HEIGHT, WIDTH)\n","#         if item_set == 'test':\n","#             preprocess_image(image_id, test_base_path, test_images_dest_path, HEIGHT, WIDTH)\n","\n","# Model evaluation\n","def get_metrics_classification(df, preds, label_columns, threshold=0.5, show_report=True):\n","  accuracy = []\n","  precision = []\n","  recall = []\n","  f_score = []\n","  for index, label in enumerate(label_columns):\n","    print('Metrics for: %s' % label)\n","    if show_report:\n","      print(classification_report(df[label], (preds[:,index] > threshold).astype(int), output_dict=False))\n","    metrics = classification_report(df[label], (preds[:,index] > threshold).astype(int), output_dict=True)\n","    accuracy.append(metrics['accuracy'])\n","    precision.append(metrics['1']['precision'])\n","    recall.append(metrics['1']['recall'])\n","    f_score.append(metrics['1']['f1-score'])\n","    \n","  print('Averaged accuracy:  %.2f' % np.mean(accuracy))\n","  print('Averaged precision: %.2f' % np.mean(precision))\n","  print('Averaged recall:    %.2f' % np.mean(recall))\n","  print('Averaged f_score:   %.2f' % np.mean(f_score))\n","\n","def plot_metrics(history, metric_list=['loss', 'dice_coef'], figsize=(22, 14)):\n","    fig, axes = plt.subplots(len(metric_list), 1, sharex='col', figsize=(22, len(metric_list)*4))\n","    axes = axes.flatten()\n","    \n","    for index, metric in enumerate(metric_list):\n","        axes[index].plot(history[metric], label='Train %s' % metric)\n","        axes[index].plot(history['val_%s' % metric], label='Validation %s' % metric)\n","        axes[index].legend(loc='best')\n","        axes[index].set_title(metric)\n","\n","    plt.xlabel('Epochs')\n","    sns.despine()\n","    plt.show()\n","\n","# Model post process\n","def post_process(probability, threshold=0.5, min_size=10000):\n","    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n","    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n","    predictions = np.zeros(probability.shape, np.float32)\n","    for c in range(1, num_component):\n","        p = (component == c)\n","        if p.sum() > min_size:\n","            predictions[p] = 1\n","    return predictions\n","\n","# Prediction evaluation\n","def get_metrics(model, target_df, df, df_images_dest_path, label_columns, tresholds, min_mask_sizes, N_CLASSES=4, seed=0, preprocessing=None, adjust_fn=None, adjust_param=None, set_name='Complete set', column_names=['Class', 'Dice', 'Dice Post']):\n","    metrics = []\n","\n","    for class_name in label_columns:\n","        metrics.append([class_name, 0, 0])\n","\n","    metrics_df = pd.DataFrame(metrics, columns=column_names)\n","    \n","    for i in range(0, df.shape[0], 500):\n","        batch_idx = list(range(i, min(df.shape[0], i + 500)))\n","        batch_set = df[batch_idx[0]: batch_idx[-1]+1]\n","        ratio = len(batch_set) / len(df)\n","\n","        generator = DataGenerator(\n","                      directory=df_images_dest_path,\n","                      dataframe=batch_set,\n","                      target_df=target_df,\n","                      batch_size=len(batch_set), \n","                      target_size=model.input_shape[1:3],\n","                      n_channels=model.input_shape[3],\n","                      n_classes=N_CLASSES,\n","                      preprocessing=preprocessing,\n","                      adjust_fn=adjust_fn,\n","                      adjust_param=adjust_param,\n","                      seed=seed,\n","                      mode='fit',\n","                      shuffle=False)\n","\n","        x, y = generator.__getitem__(0)\n","        preds = model.predict(x)\n","        \n","        for class_index in range(N_CLASSES):\n","            class_score = []\n","            class_score_post = []\n","            mask_class = y[..., class_index]\n","            pred_class = preds[..., class_index]\n","            for index in range(len(batch_idx)):\n","                sample_mask = mask_class[index, ]\n","                sample_pred = pred_class[index, ]\n","                sample_pred_post = post_process(sample_pred, threshold=tresholds[class_index], min_size=min_mask_sizes[class_index])\n","                if (sample_mask.sum() == 0) & (sample_pred.sum() == 0):\n","                    dice_score = 1.\n","                else:\n","                    dice_score = dice_coefficient(sample_pred, sample_mask)\n","                if (sample_mask.sum() == 0) & (sample_pred_post.sum() == 0):\n","                    dice_score_post = 1.\n","                else:\n","                    dice_score_post = dice_coefficient(sample_pred_post, sample_mask)\n","                class_score.append(dice_score)\n","                class_score_post.append(dice_score_post)\n","            metrics_df.loc[metrics_df[column_names[0]] == label_columns[class_index], column_names[1]] += np.mean(class_score) * ratio\n","            metrics_df.loc[metrics_df[column_names[0]] == label_columns[class_index], column_names[2]] += np.mean(class_score_post) * ratio\n","\n","    metrics_df = metrics_df.append({column_names[0]:set_name, column_names[1]:np.mean(metrics_df[column_names[1]].values), column_names[2]:np.mean(metrics_df[column_names[2]].values)}, ignore_index=True).set_index(column_names[0])\n","    \n","    return metrics_df\n","\n","def inspect_predictions(df, image_ids, images_dest_path, pred_col=None, label_col='EncodedPixels', title_col='Image_Label', img_shape=(525, 350), figsize=(22, 6)):\n","    if pred_col:\n","        for sample in image_ids:\n","            sample_df = df[df['image'] == sample]\n","            fig, axes = plt.subplots(2, 5, figsize=figsize)\n","            img = cv2.imread(images_dest_path + sample_df['image'].values[0])\n","            img = cv2.resize(img, img_shape)\n","            axes[0][0].imshow(img)\n","            axes[1][0].imshow(img)\n","            axes[0][0].set_title('Label', fontsize=16)\n","            axes[1][0].set_title('Predicted', fontsize=16)\n","            axes[0][0].axis('off')\n","            axes[1][0].axis('off')\n","            for i in range(4):\n","                mask = sample_df[label_col].values[i]\n","                try:\n","                    math.isnan(mask)\n","                    mask = np.zeros((img_shape[1], img_shape[0]))\n","                except:\n","                    mask = rle_decode(mask)\n","                axes[0][i+1].imshow(mask)\n","                axes[1][i+1].imshow(rle2mask(sample_df[pred_col].values[i], img.shape))\n","                axes[0][i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","                axes[1][i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","                axes[0][i+1].axis('off')\n","                axes[1][i+1].axis('off')\n","    else:\n","        for sample in image_ids:\n","            sample_df = df[df['image'] == sample]\n","            fig, axes = plt.subplots(1, 5, figsize=figsize)\n","            img = cv2.imread(images_dest_path + sample_df['image'].values[0])\n","            img = cv2.resize(img, img_shape)\n","            axes[0].imshow(img)\n","            axes[0].set_title('Original', fontsize=16)\n","            axes[0].axis('off')\n","            for i in range(4):\n","                mask = sample_df[label_col].values[i]\n","                try:\n","                    math.isnan(mask)\n","                    mask = np.zeros((img_shape[1], img_shape[0]))\n","                except:\n","                    mask = rle_decode(mask, shape=(img_shape[1], img_shape[0]))\n","                axes[i+1].imshow(mask)\n","                axes[i+1].set_title(sample_df[title_col].values[i], fontsize=18)\n","                axes[i+1].axis('off')\n","                \n","# Model tunning\n","def classification_tunning(y_true, y_pred, label_columns, threshold_grid=np.arange(0, 1, .01), column_names=['Class', 'Threshold', 'Score'], print_score=True):\n","  metrics = []\n","  for label in label_columns:\n","      for threshold in threshold_grid:\n","          metrics.append([label, threshold, 0])\n","\n","  metrics_df = pd.DataFrame(metrics, columns=column_names)\n","  for index, label in enumerate(label_columns):\n","      for thr in threshold_grid:\n","          metrics_df.loc[(metrics_df[column_names[0]] == label) & (metrics_df[column_names[1]] == thr) , column_names[2]] = accuracy_score(y_true[:,index], (y_pred[:,index] > thr).astype(int))\n","\n","  best_tresholds = []\n","  best_scores = []\n","  for index, label in enumerate(label_columns):\n","    metrics_df_lbl = metrics_df[metrics_df[column_names[0]] == label_columns[index]]\n","    optimal_values_lbl = metrics_df_lbl.loc[metrics_df_lbl[column_names[2]].idxmax()].values\n","    best_tresholds.append(optimal_values_lbl[1])\n","    best_scores.append(optimal_values_lbl[2])\n","\n","  if print_score:\n","    for index, label in enumerate(label_columns):\n","      print('%s treshold=%.2f Score=%.3f' % (label, best_tresholds[index], best_scores[index]))\n","\n","  return best_tresholds\n","\n","def segmentation_tunning(model, target_df, df, df_images_dest_path, label_columns, mask_grid, threshold_grid=np.arange(0, 1, .01), N_CLASSES=4, preprocessing=None, adjust_fn=None, adjust_param=None, seed=0, column_names=['Class', 'Threshold', 'Mask size', 'Dice'], print_score=True):\n","    metrics = []\n","\n","    for label in label_columns:\n","        for threshold in threshold_grid:\n","            for mask_size in mask_grid:\n","                metrics.append([label, threshold, mask_size, 0])\n","\n","    metrics_df = pd.DataFrame(metrics, columns=column_names)\n","\n","    for i in range(0, df.shape[0], 500):\n","        batch_idx = list(range(i, min(df.shape[0], i + 500)))\n","        batch_set = df[batch_idx[0]: batch_idx[-1]+1]\n","        ratio = len(batch_set) / len(df)\n","\n","        generator = DataGenerator(\n","                      directory=df_images_dest_path,\n","                      dataframe=batch_set,\n","                      target_df=target_df,\n","                      batch_size=len(batch_set), \n","                      target_size=model.input_shape[1:3],\n","                      n_channels=model.input_shape[3],\n","                      n_classes=N_CLASSES,\n","                      preprocessing=preprocessing,\n","                      adjust_fn=adjust_fn,\n","                      adjust_param=adjust_param,\n","                      seed=seed,\n","                      mode='fit',\n","                      shuffle=False)\n","\n","        x, y = generator.__getitem__(0)\n","        preds = model.predict(x)\n","\n","        for class_index, label in enumerate(label_columns):\n","            class_score = []\n","            label_class = y[..., class_index]\n","            pred_class = preds[..., class_index]\n","            for threshold in threshold_grid:\n","                for mask_size in mask_grid:\n","                    mask_score = []\n","                    for index in range(len(batch_idx)):\n","                        label_mask = label_class[index, ]\n","                        pred_mask = pred_class[index, ]\n","                        pred_mask = post_process(pred_mask, threshold=threshold, min_size=mask_size)\n","                        dice_score = dice_coefficient(pred_mask, label_mask)\n","                        if (pred_mask.sum() == 0) & (label_mask.sum() == 0):\n","                            dice_score = 1.\n","                        mask_score.append(dice_score)\n","                    metrics_df.loc[(metrics_df[column_names[0]] == label) & (metrics_df[column_names[1]] == threshold) & \n","                                   (metrics_df[column_names[2]] == mask_size), column_names[3]] += np.mean(mask_score) * ratio\n","                    \n","    best_tresholds = []\n","    best_masks = []\n","    best_dices = []\n","    for index, label in enumerate(label_columns):\n","        metrics_df_lbl = metrics_df[metrics_df[column_names[0]] == label_columns[index]]\n","        optimal_values_lbl = metrics_df_lbl.loc[metrics_df_lbl[column_names[3]].idxmax()].values\n","        best_tresholds.append(optimal_values_lbl[1])\n","        best_masks.append(optimal_values_lbl[2])\n","        best_dices.append(optimal_values_lbl[3])\n","\n","    if print_score:\n","        for index, name in enumerate(label_columns):\n","            print('%s treshold=%.2f mask size=%d Dice=%.3f' % (name, best_tresholds[index], best_masks[index], best_dices[index]))\n","            \n","    return best_tresholds, best_masks\n","\n","# Data generator\n","class DataGenerator(Sequence):\n","    def __init__(self, dataframe, directory, batch_size, n_channels, target_size,  n_classes, \n","                 mode='fit', target_df=None, shuffle=True, preprocessing=None, augmentation=None, adjust_fn=None, adjust_param=None, seed=0):\n","        \n","        self.batch_size = batch_size\n","        self.dataframe = dataframe\n","        self.mode = mode\n","        self.directory = directory\n","        self.target_df = target_df\n","        self.target_size = target_size\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.preprocessing = preprocessing\n","        self.augmentation = augmentation\n","        self.adjust_fn = adjust_fn\n","        self.adjust_param = adjust_param\n","        self.seed = seed\n","        self.mask_shape = (1400, 2100)\n","        self.list_IDs = self.dataframe.index\n","        \n","        if self.seed is not None:\n","            np.random.seed(self.seed)\n","        \n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        return len(self.list_IDs) // self.batch_size\n","\n","    def __getitem__(self, index):\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n","        X = self.__generate_X(list_IDs_batch)\n","        \n","        if self.mode == 'fit':\n","            Y = self.__generate_Y(list_IDs_batch)\n","            \n","            if self.augmentation:\n","                X, Y = self.__augment_batch(X, Y)\n","            \n","            return X, Y\n","        \n","        elif self.mode == 'predict':\n","            return X\n","        \n","    def on_epoch_end(self):\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","    \n","    def __generate_X(self, list_IDs_batch):\n","        X = np.empty((self.batch_size, *self.target_size, self.n_channels))\n","        \n","        for i, ID in enumerate(list_IDs_batch):\n","            img_name = self.dataframe['image'].loc[ID]\n","            img_path = self.directory + img_name\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            \n","            if (not self.adjust_fn is None) & (not self.adjust_param is None):\n","                img = self.adjust_fn(img, self.adjust_param)\n","\n","            if self.preprocessing:\n","                img = self.preprocessing(img)\n","                \n","            X[i,] = img\n","\n","        return X\n","    \n","    def __generate_Y(self, list_IDs_batch):\n","        Y = np.empty((self.batch_size, *self.target_size, self.n_classes), dtype=int)\n","        \n","        for i, ID in enumerate(list_IDs_batch):\n","            img_name = self.dataframe['image'].loc[ID]\n","            image_df = self.target_df[self.target_df['image'] == img_name]\n","            rles = image_df['EncodedPixels'].values\n","            masks = build_masks(rles, input_shape=self.mask_shape, reshape=self.target_size)\n","            Y[i, ] = masks\n","\n","        return Y\n","    \n","    def __augment_batch(self, X_batch, Y_batch):\n","        for i in range(X_batch.shape[0]):\n","            X_batch[i, ], Y_batch[i, ] = self.__random_transform(X_batch[i, ], Y_batch[i, ])\n","        \n","        return X_batch, Y_batch\n","    \n","    def __random_transform(self, X, Y):\n","        composed = self.augmentation(image=X, mask=Y)\n","        X_aug = composed['image']\n","        Y_aug = composed['mask']\n","        \n","        return X_aug, Y_aug"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Segmentation Models: using `keras` framework.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"puocGnPOx__4","colab_type":"code","colab":{}},"source":["seed = 0\n","seed_everything(seed)\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JCVDcAP0Hq0P","colab_type":"code","colab":{}},"source":["base_path = '/content/drive/My Drive/Colab Notebooks/[Kaggle] Understanding Clouds from Satellite Images/'\n","data_path = base_path + 'Data/'\n","model_base_path = base_path + 'Models/files/segmentation/'\n","train_path = data_path + 'train.csv'\n","kfold_set_path = data_path + '5-fold.csv'\n","train_images_dest_path = 'train_images384x480/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6SnKKLczHdqn","colab_type":"text"},"source":["### Load data"]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"pH6kKJKoHdqo","colab_type":"code","outputId":"d30300be-21b0-4494-cc12-d4c23e6827c8","executionInfo":{"status":"ok","timestamp":1571338492865,"user_tz":180,"elapsed":54795,"user":{"displayName":"Dimitre Oliveira","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHzrYFhikwGj5HS4HCH2B5iUmYoPpm1AFV6OcFBA=s64","userId":"06256612867315483887"}},"colab":{"base_uri":"https://localhost:8080/","height":414}},"source":["train = pd.read_csv(train_path)\n","\n","kfold_set = pd.read_csv(kfold_set_path)\n","X_train = kfold_set[kfold_set['fold_4'] == 'train']\n","X_val = kfold_set[kfold_set['fold_4'] == 'validation']\n","\n","print('Compete set samples:', len(train))\n","print('Train samples: ', len(X_train))\n","print('Validation samples: ', len(X_val))\n","\n","# Preprocecss data\n","train['image'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n","\n","display(X_train.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Compete set samples: 22184\n","Train samples:  4421\n","Validation samples:  1104\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>Fish</th>\n","      <th>Flower</th>\n","      <th>Sugar</th>\n","      <th>Gravel</th>\n","      <th>Fish_mask</th>\n","      <th>Flower_mask</th>\n","      <th>Gravel_mask</th>\n","      <th>Sugar_mask</th>\n","      <th>fold_0</th>\n","      <th>fold_1</th>\n","      <th>fold_2</th>\n","      <th>fold_3</th>\n","      <th>fold_4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0011165.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n","      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>train</td>\n","      <td>train</td>\n","      <td>validation</td>\n","      <td>train</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>002be4f.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n","      <td>1339279 519 1340679 519 1342079 519 1343479 51...</td>\n","      <td>NaN</td>\n","      <td>67495 350 68895 350 70295 350 71695 350 73095 ...</td>\n","      <td>train</td>\n","      <td>train</td>\n","      <td>validation</td>\n","      <td>train</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0035239.jpg</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>100812 462 102212 462 103612 462 105012 462 10...</td>\n","      <td>65400 380 66800 380 68200 380 69600 380 71000 ...</td>\n","      <td>NaN</td>\n","      <td>validation</td>\n","      <td>train</td>\n","      <td>train</td>\n","      <td>train</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>003994e.jpg</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2367966 18 2367985 2 2367993 8 2368002 62 2369...</td>\n","      <td>NaN</td>\n","      <td>353317 416 354717 416 356117 416 357517 416 35...</td>\n","      <td>28011 489 29411 489 30811 489 32211 489 33611 ...</td>\n","      <td>train</td>\n","      <td>train</td>\n","      <td>validation</td>\n","      <td>train</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>00498ec.jpg</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>326420 552 327820 552 329220 552 330620 552 33...</td>\n","      <td>NaN</td>\n","      <td>train</td>\n","      <td>train</td>\n","      <td>train</td>\n","      <td>validation</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         image  Fish  Flower  Sugar  ...  fold_1      fold_2      fold_3 fold_4\n","0  0011165.jpg     1       1      0  ...   train  validation       train  train\n","1  002be4f.jpg     1       1      1  ...   train  validation       train  train\n","3  0035239.jpg     0       1      0  ...   train       train       train  train\n","4  003994e.jpg     1       0      1  ...   train  validation       train  train\n","5  00498ec.jpg     0       0      0  ...   train       train  validation  train\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"xyPzEBHGHdqr","colab_type":"text"},"source":["# Model parameters"]},{"cell_type":"code","metadata":{"id":"BBNl0qkSHdqs","colab_type":"code","colab":{}},"source":["BACKBONE = 'densenet169'\n","BATCH_SIZE = 8\n","EPOCHS = 30\n","LEARNING_RATE = 1e-3\n","HEIGHT = 384\n","WIDTH = 480\n","CHANNELS = 3\n","N_CLASSES = 4\n","ES_PATIENCE = 5\n","RLROP_PATIENCE = 3\n","DECAY_DROP = 0.2\n","model_name = '50-[K-Fold5]unet_%s_%sx%s' % (BACKBONE, HEIGHT, WIDTH)\n","model_path = model_base_path + '%s.h5' % (model_name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":false,"id":"YXFIjNcKHdqv","colab_type":"code","colab":{}},"source":["preprocessing = sm.get_preprocessing(BACKBONE)\n","\n","augmentation = albu.Compose([albu.HorizontalFlip(p=0.5),\n","                             albu.VerticalFlip(p=0.5),\n","                             albu.GridDistortion(p=0.5),\n","                             albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, \n","                                                   shift_limit=0.1, border_mode=0, p=0.5),\n","                             albu.OpticalDistortion(p=0.5, distort_limit=2, shift_limit=0.5)\n","                            ])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vaGBD0NAHdq7","colab_type":"text"},"source":["### Data generator"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"yYQxaJVLHdq8","colab_type":"code","cellView":"both","colab":{}},"source":["train_generator = DataGenerator(\n","                  directory=train_images_dest_path,\n","                  dataframe=X_train,\n","                  target_df=train,\n","                  batch_size=BATCH_SIZE,\n","                  target_size=(HEIGHT, WIDTH),\n","                  n_channels=CHANNELS,\n","                  n_classes=N_CLASSES,\n","                  preprocessing=preprocessing,\n","                  augmentation=augmentation,\n","                  seed=seed)\n","\n","valid_generator = DataGenerator(\n","                  directory=train_images_dest_path,\n","                  dataframe=X_val,\n","                  target_df=train,\n","                  batch_size=BATCH_SIZE, \n","                  target_size=(HEIGHT, WIDTH),\n","                  n_channels=CHANNELS,\n","                  n_classes=N_CLASSES,\n","                  preprocessing=preprocessing,\n","                  seed=seed)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Idm7ex1GHdq_","colab_type":"text"},"source":["# Model"]},{"cell_type":"code","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"id":"OqorBKOuHdrA","colab_type":"code","colab":{}},"source":["model = sm.Unet(backbone_name=BACKBONE, \n","                encoder_weights='imagenet',\n","                classes=N_CLASSES,\n","                activation='sigmoid',\n","                input_shape=(HEIGHT, WIDTH, CHANNELS))\n","\n","checkpoint = ModelCheckpoint(model_path, monitor='val_loss', mode='min', save_best_only=True)\n","es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n","rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, verbose=1)\n","\n","metric_list = [dice_coef, sm.metrics.iou_score, sm.metrics.f1_score]\n","callback_list = [checkpoint, es, rlrop]\n","optimizer = RAdam(learning_rate=LEARNING_RATE, warmup_proportion=0.1)\n","\n","model.compile(optimizer=optimizer, loss=sm.losses.bce_dice_loss, metrics=metric_list)\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"id":"bD00H9izHdrD","colab_type":"code","outputId":"cda674e6-4943-4306-ffe7-fb336bf6cbb7","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["STEP_SIZE_TRAIN = len(X_train)//BATCH_SIZE\n","STEP_SIZE_VALID = len(X_val)//BATCH_SIZE\n","\n","history = model.fit_generator(generator=train_generator,\n","                              steps_per_epoch=STEP_SIZE_TRAIN,\n","                              validation_data=valid_generator,\n","                              validation_steps=STEP_SIZE_VALID,\n","                              callbacks=callback_list,\n","                              epochs=EPOCHS,\n","                              verbose=1).history"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/30\n","552/552 [==============================] - 983s 2s/step - loss: 1.0899 - dice_coef: 0.3497 - iou_score: 0.2101 - f1-score: 0.3373 - val_loss: 1.6033 - val_dice_coef: 0.3193 - val_iou_score: 0.1750 - val_f1-score: 0.2854\n","Epoch 2/30\n","552/552 [==============================] - 904s 2s/step - loss: 0.9412 - dice_coef: 0.4158 - iou_score: 0.2613 - f1-score: 0.4032 - val_loss: 1.3307 - val_dice_coef: 0.3328 - val_iou_score: 0.1852 - val_f1-score: 0.2989\n","Epoch 3/30\n","552/552 [==============================] - 902s 2s/step - loss: 0.9180 - dice_coef: 0.4338 - iou_score: 0.2765 - f1-score: 0.4215 - val_loss: 0.9394 - val_dice_coef: 0.4967 - val_iou_score: 0.3184 - val_f1-score: 0.4676\n","Epoch 4/30\n","552/552 [==============================] - 901s 2s/step - loss: 0.9030 - dice_coef: 0.4435 - iou_score: 0.2845 - f1-score: 0.4305 - val_loss: 0.9343 - val_dice_coef: 0.4830 - val_iou_score: 0.3038 - val_f1-score: 0.4510\n","Epoch 5/30\n","552/552 [==============================] - 902s 2s/step - loss: 0.9014 - dice_coef: 0.4451 - iou_score: 0.2849 - f1-score: 0.4310 - val_loss: 0.9780 - val_dice_coef: 0.4631 - val_iou_score: 0.2891 - val_f1-score: 0.4352\n","Epoch 6/30\n","552/552 [==============================] - 903s 2s/step - loss: 0.8917 - dice_coef: 0.4515 - iou_score: 0.2908 - f1-score: 0.4382 - val_loss: 1.1103 - val_dice_coef: 0.4423 - val_iou_score: 0.2581 - val_f1-score: 0.3898\n","Epoch 7/30\n","552/552 [==============================] - 903s 2s/step - loss: 0.8901 - dice_coef: 0.4540 - iou_score: 0.2924 - f1-score: 0.4399 - val_loss: 0.9048 - val_dice_coef: 0.5019 - val_iou_score: 0.3312 - val_f1-score: 0.4802\n","Epoch 8/30\n","552/552 [==============================] - 903s 2s/step - loss: 0.8824 - dice_coef: 0.4588 - iou_score: 0.2963 - f1-score: 0.4442 - val_loss: 0.9787 - val_dice_coef: 0.4904 - val_iou_score: 0.3057 - val_f1-score: 0.4538\n","Epoch 9/30\n","552/552 [==============================] - 902s 2s/step - loss: 0.8806 - dice_coef: 0.4597 - iou_score: 0.2974 - f1-score: 0.4457 - val_loss: 0.9834 - val_dice_coef: 0.4350 - val_iou_score: 0.2710 - val_f1-score: 0.4127\n","Epoch 10/30\n","552/552 [==============================] - 901s 2s/step - loss: 0.8703 - dice_coef: 0.4643 - iou_score: 0.3023 - f1-score: 0.4514 - val_loss: 1.0348 - val_dice_coef: 0.4399 - val_iou_score: 0.2835 - val_f1-score: 0.4251\n","\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","Epoch 11/30\n","552/552 [==============================] - 902s 2s/step - loss: 0.8381 - dice_coef: 0.4809 - iou_score: 0.3166 - f1-score: 0.4682 - val_loss: 0.8027 - val_dice_coef: 0.5246 - val_iou_score: 0.3403 - val_f1-score: 0.4928\n","Epoch 12/30\n","552/552 [==============================] - 903s 2s/step - loss: 0.8280 - dice_coef: 0.4927 - iou_score: 0.3259 - f1-score: 0.4783 - val_loss: 0.8028 - val_dice_coef: 0.5311 - val_iou_score: 0.3442 - val_f1-score: 0.4959\n","Epoch 13/30\n","552/552 [==============================] - 903s 2s/step - loss: 0.8255 - dice_coef: 0.4942 - iou_score: 0.3274 - f1-score: 0.4794 - val_loss: 0.8417 - val_dice_coef: 0.5099 - val_iou_score: 0.3337 - val_f1-score: 0.4851\n","Epoch 14/30\n","552/552 [==============================] - 903s 2s/step - loss: 0.8261 - dice_coef: 0.4946 - iou_score: 0.3267 - f1-score: 0.4791 - val_loss: 0.8058 - val_dice_coef: 0.5328 - val_iou_score: 0.3519 - val_f1-score: 0.5025\n","\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n","Epoch 15/30\n","552/552 [==============================] - 900s 2s/step - loss: 0.8129 - dice_coef: 0.5021 - iou_score: 0.3333 - f1-score: 0.4866 - val_loss: 0.7803 - val_dice_coef: 0.5452 - val_iou_score: 0.3593 - val_f1-score: 0.5121\n","Epoch 16/30\n","552/552 [==============================] - 902s 2s/step - loss: 0.8097 - dice_coef: 0.5054 - iou_score: 0.3365 - f1-score: 0.4894 - val_loss: 0.7748 - val_dice_coef: 0.5494 - val_iou_score: 0.3617 - val_f1-score: 0.5136\n","Epoch 17/30\n","552/552 [==============================] - 902s 2s/step - loss: 0.8051 - dice_coef: 0.5056 - iou_score: 0.3379 - f1-score: 0.4913 - val_loss: 0.7676 - val_dice_coef: 0.5502 - val_iou_score: 0.3652 - val_f1-score: 0.5187\n","Epoch 18/30\n","552/552 [==============================] - 900s 2s/step - loss: 0.8015 - dice_coef: 0.5103 - iou_score: 0.3410 - f1-score: 0.4947 - val_loss: 0.7728 - val_dice_coef: 0.5493 - val_iou_score: 0.3629 - val_f1-score: 0.5159\n","Epoch 19/30\n","552/552 [==============================] - 906s 2s/step - loss: 0.8038 - dice_coef: 0.5093 - iou_score: 0.3401 - f1-score: 0.4934 - val_loss: 0.7661 - val_dice_coef: 0.5504 - val_iou_score: 0.3675 - val_f1-score: 0.5210\n","Epoch 20/30\n","552/552 [==============================] - 901s 2s/step - loss: 0.8011 - dice_coef: 0.5091 - iou_score: 0.3395 - f1-score: 0.4932 - val_loss: 0.7646 - val_dice_coef: 0.5525 - val_iou_score: 0.3681 - val_f1-score: 0.5215\n","Epoch 21/30\n","552/552 [==============================] - 896s 2s/step - loss: 0.8006 - dice_coef: 0.5100 - iou_score: 0.3408 - f1-score: 0.4942 - val_loss: 0.7700 - val_dice_coef: 0.5513 - val_iou_score: 0.3673 - val_f1-score: 0.5208\n","Epoch 22/30\n","552/552 [==============================] - 893s 2s/step - loss: 0.8001 - dice_coef: 0.5115 - iou_score: 0.3413 - f1-score: 0.4954 - val_loss: 0.7652 - val_dice_coef: 0.5519 - val_iou_score: 0.3664 - val_f1-score: 0.5205\n","Epoch 23/30\n","552/552 [==============================] - 892s 2s/step - loss: 0.7959 - dice_coef: 0.5138 - iou_score: 0.3447 - f1-score: 0.4992 - val_loss: 0.7606 - val_dice_coef: 0.5521 - val_iou_score: 0.3714 - val_f1-score: 0.5264\n","Epoch 24/30\n","552/552 [==============================] - 897s 2s/step - loss: 0.7929 - dice_coef: 0.5143 - iou_score: 0.3465 - f1-score: 0.5008 - val_loss: 0.7631 - val_dice_coef: 0.5539 - val_iou_score: 0.3684 - val_f1-score: 0.5214\n","Epoch 25/30\n"," 43/552 [=>............................] - ETA: 12:46 - loss: 0.8103 - dice_coef: 0.5096 - iou_score: 0.3394 - f1-score: 0.4938"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2-TKAjVxFy0g","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"0-PPEeGGHdrG","colab_type":"text"},"source":["## Model loss graph"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"id":"F1uV9E1RHdrH","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","plot_metrics(history, metric_list=['loss', 'dice_coef', 'iou_score', 'f1-score'])"],"execution_count":0,"outputs":[]}]}